{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "uxQdiEfdUtIu"
      },
      "outputs": [],
      "source": [
        "# !pip install pycuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "rKMur4pFU0Ok"
      },
      "outputs": [],
      "source": [
        "kernel_source = \"\"\"\n",
        "#define TILE_SIZE 4 //This can be changes to 8, 12 or 32 depending on the GPU\n",
        "\n",
        "__global__ void tiled_mat_mul (int *a, int *b, int *c, int N1, int N2, int N3)\n",
        "{\n",
        "  //shared memory tile blocks for A and B\n",
        "\n",
        "  __shared__ int tileA[TILE_SIZE][TILE_SIZE];\n",
        "  __shared__ int tileB[TILE_SIZE][TILE_SIZE];\n",
        "\n",
        "\n",
        "  int tx = threadIdx.x;\n",
        "  int ty = threadIdx.y;\n",
        "\n",
        "// working on C[i,j]\n",
        "\n",
        "  int row =blockIdx.y*TILE_SIZE + threadIdx.y;\n",
        "  int col =blockIdx.x*TILE_SIZE + threadIdx.x;\n",
        "\n",
        "  int temp = 0;\n",
        "\n",
        "  //Loop over all tiles  required to compute the C[row][col]\n",
        "\n",
        "  for (int phase=0 ; phase<(N2+TILE_SIZE-1)/TILE_SIZE; phase++)\n",
        "  {\n",
        "\n",
        "    //load tiles into shared memory\n",
        "    if ((row<N1)&& ((phase*TILE_SIZE+tx)<N2))\n",
        "      tileA[ty][tx]=a[(row)*N2+phase*TILE_SIZE+tx];\n",
        "    else\n",
        "      tileA[ty][tx]=0;\n",
        "\n",
        "    if ( ((phase*TILE_SIZE+ty)<N2)&&(col<N3))\n",
        "        tileB[ty][tx]=b[(phase*TILE_SIZE+ty)*N3+col];\n",
        "      else\n",
        "        tileB[ty][tx]=0;\n",
        "    __syncthreads();\n",
        "\n",
        "    //Dot product\n",
        "    for (int k=0;k<TILE_SIZE;k++)\n",
        "    {\n",
        "      temp+=tileA[ty][k]*tileB[k][tx];\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "  }\n",
        "\n",
        "  //assigning the calculated value\n",
        "  if ((row<N1)&&(col<N3))\n",
        "  {\n",
        "    c[row*N3+col]=temp;\n",
        "  }\n",
        "\n",
        "  }\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "dje4PP52U0S9"
      },
      "outputs": [],
      "source": [
        "import pycuda.autoinit\n",
        "import numpy as np\n",
        "import pycuda.driver as cuda\n",
        "from pycuda.compiler import SourceModule\n",
        "import time\n",
        "TILE_SIZE= 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd-KipaCU0YY",
        "outputId": "afae4883-1371-42f6-9e24-e629920892b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling the CUDA Kernel....\n",
            "kernel compiled successfully\n"
          ]
        }
      ],
      "source": [
        "print(\"Compiling the CUDA Kernel....\")\n",
        "#compile the cuda\n",
        "\n",
        "mode =  SourceModule(kernel_source)\n",
        "\n",
        "#Get the compiler function from the compile module\n",
        "mat_mul= mode.get_function('tiled_mat_mul')\n",
        "\n",
        "\n",
        "print(\"kernel compiled successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the matirices for A and B\n",
        "\n",
        "M=100\n",
        "N=10\n",
        "P=100\n",
        "np.random.seed(1)\n",
        "mat1 = np.random.randint(0, 10, size=(M,N)).astype(np.int32)\n",
        "mat2= np.random.randint(0, 10, size=(N,P)).astype(np.int32)\n",
        "mat3= np.empty_like(np.zeros((M,P))).astype(np.int32)\n",
        "\n",
        "\n",
        "print(f\"The matrices are loaded with shape {mat1.shape} {mat2.shape} {mat3.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xj6Wkn4Drgtt",
        "outputId": "bdbad3de-162d-4e85-cdac-37b30c80d18e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The matrices are loaded with shape (100, 10) (10, 100) (100, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initilize the variables in GPU and copy the matrices to GPU\n",
        "a=cuda.mem_alloc(mat1.nbytes)\n",
        "b=cuda.mem_alloc(mat2.nbytes)\n",
        "c=cuda.mem_alloc(mat3.nbytes)\n",
        "\n",
        "print(\"memory on gpu allocated successfully\")\n",
        "\n",
        "#Copy the matrices to GPU\n",
        "cuda.memcpy_htod(a,mat1)\n",
        "cuda.memcpy_htod(b,mat2)\n",
        "cuda.memcpy_htod(c,mat3)\n",
        "\n",
        "print(\"Copying the matrices to the gpu was successful\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp_JHS93rg8n",
        "outputId": "ca00e37a-0fdc-4c3a-d9c1-305c9a16ea9b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "memory on gpu allocated successfully\n",
            "Copying the matrices to the gpu was successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Execution in GPU\n",
        "#Define the Block size and GridSize\n",
        "BLOCK_SIZE=(TILE_SIZE,TILE_SIZE,1)\n",
        "GRID_SIZE=((P + TILE_SIZE - 1) // TILE_SIZE,(M+TILE_SIZE-1)//TILE_SIZE)\n",
        "\n",
        "#Stat the timer\n",
        "start_time_gpu=time.time()\n",
        "\n",
        "#Execute in gpu\n",
        "mat_mul(a,b,c,np.int32(M),np.int32(N),np.int32(P),block=BLOCK_SIZE,grid=GRID_SIZE)\n",
        "\n",
        "#Wait for the GPU to Finish the Process\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "#stop the timer\n",
        "end_time_gpu=time.time()\n",
        "\n",
        "time_taken_for_gpu=end_time_gpu-start_time_gpu\n",
        "\n",
        "print(f'Execution completed successfull in GPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJDZ2-Xcrg_a",
        "outputId": "9c0ad2bb-d365-4c29-d12e-f2b78b4afa37"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution completed successfull in GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYkBqSrAU0dx",
        "outputId": "093f77e7-d18f-41ed-b0a4-87d18c0d2bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The matrix is copied successfully back to CPU\n"
          ]
        }
      ],
      "source": [
        "#Copy the resultant matrix back to CPU\n",
        "cuda.memcpy_dtoh(mat3,c)\n",
        "\n",
        "print(\"The matrix is copied successfully back to CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "-k_ocdzMU0jf"
      },
      "outputs": [],
      "source": [
        "#Matrix Multiplication in\n",
        "\n",
        "#Stat the timer\n",
        "start_time_cpu=time.time()\n",
        "\n",
        "#Execute in cpu\n",
        "mat4=mat1@mat2\n",
        "\n",
        "#stop the timer\n",
        "end_time_cpu=time.time()\n",
        "\n",
        "time_taken_for_cpu=end_time_cpu-start_time_cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4QdfQJUU0qF",
        "outputId": "3218eef5-f76e-48a3-e250-1123e8540ce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken in GPU is 0.0002372264862060547 \n",
            "Time taken in CPU is 0.0003228187561035156\n"
          ]
        }
      ],
      "source": [
        "print(f\"Time taken in GPU is {time_taken_for_gpu} \\nTime taken in CPU is {time_taken_for_cpu}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}